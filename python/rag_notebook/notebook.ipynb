{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e5467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client.http import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead4a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(\n",
    "    url=\"\",\n",
    "    api_key=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd89a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_27686/1465433335.py:1: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant_client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create collection reponse: <qdrant_client.qdrant_client.QdrantClient object at 0x7f89682aba30>\n"
     ]
    }
   ],
   "source": [
    "qdrant_client.recreate_collection(\n",
    "    collection_name=\"mycollection\",\n",
    "    vectors_config=models.VectorParams(size=1024, distance=models.Distance.COSINE),\n",
    ")\n",
    "print(\"Create collection reponse:\", qdrant_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a375339f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection info: status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=0 points_count=0 segments_count=2 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=1024, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None) payload_schema={}\n"
     ]
    }
   ],
   "source": [
    "collection_info = qdrant_client.get_collection(collection_name=\"mycollection\")\n",
    "\n",
    "print(\"Collection info:\", collection_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd3ef1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "fulltext = \"\"\n",
    "with pdfplumber.open(\"english_text.pdf\") as pdf:\n",
    "    # loop over all the pages\n",
    "    for page in pdf.pages:\n",
    "        fulltext += page.extract_text()\n",
    "\n",
    "# print(fulltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbfc98e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good day Mr. Boss\n",
      "I hope this email finds you well.\n",
      "In this email I would like to propose a new idea concerning our way of working together as a\n",
      "team.\n",
      "Last weekend, I attended a TED talk as a guest. One of the speakers presented this new way of\n",
      "working together, that is based on sharing more responsibility between team members.\n",
      "The results of her studies are quite impressive\n",
      "---\n",
      " The overall performance as well as the personal\n",
      "well being and work-life balance of each member of the test group significantly improved over\n",
      "the test periode. The whole staff, including the team lead, was wholeheartedly convinced by the\n",
      "new method and the enterprise in question even launched an initiative to implement this new\n",
      "way of doing things for the whole company\n",
      "---\n",
      "\n",
      "So what does this method include?\n",
      "- flat hierarchies\n",
      "- sharing responsibilities and including people during decision making\n",
      "- open and blame-free environment\n",
      "We have such a talented, diverse and motivated team and I am sure we could reach the same\n",
      "results that were presented during the TED talk\n",
      "---\n",
      " By including the team and providing them with\n",
      "a sense of ownership and stake in the whole process, we would enhance our productivity, I am\n",
      "sure of it!\n",
      "This method might be a challenge to the current hierarchical structure in our company but I\n",
      "nonetheless wanted to put this idea forward. I am adding the recording of the talk in the\n",
      "attachments so you can take a look for yourself\n",
      "---\n",
      " I hope you will consider it with an open mind\n",
      "and I would be looking forward to discussing it during our next team meeting.\n",
      "Please let me know what you think of my proposal and if you prefer an additional one-on-one\n",
      "session concerning this topic, please let me know.\n",
      "Thank you for your consideration.\n",
      "Sincerely,\n",
      "Etienne Baumgartner\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "text = fulltext\n",
    "\n",
    "chunks = []\n",
    "while len(text) > 500:\n",
    "    last_period_index = text[:500].rfind('.')\n",
    "    if last_period_index == -1:\n",
    "        last_period_index = 500\n",
    "    chunks.append(text[:last_period_index])\n",
    "    text = text[last_period_index+1:]\n",
    "chunks.append(text)\n",
    "\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89260018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Loading model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\"dunzhang/stella_en_1.5B_v5\", trust_remote_code=True, device='cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "996e386e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n",
      "(1024,)\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http.models import PointStruct\n",
    "\n",
    "\n",
    "\n",
    "points = []\n",
    "i = 1\n",
    "for chunk in chunks:\n",
    "    i += 1\n",
    "    doc_embeddings = model.encode(chunk)\n",
    "    print(doc_embeddings.shape)\n",
    "    points.append(PointStruct(id=i, vector=doc_embeddings.tolist(), payload={\"text\": chunk}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "019a6475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation info: operation_id=0 status=<UpdateStatus.COMPLETED: 'completed'>\n"
     ]
    }
   ],
   "source": [
    "operation_info = qdrant_client.upsert(\n",
    "    collection_name=\"mycollection\",\n",
    "    wait=True,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "print(\"Operation info:\", operation_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c53fa",
   "metadata": {},
   "source": [
    "# Chat functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09d5c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "def create_answer_with_context(query):\n",
    "    query_embeddings = model.encode(query)\n",
    "\n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=\"mycollection\",\n",
    "        query_vector=query_embeddings, \n",
    "        limit=5\n",
    "    )\n",
    "\n",
    "    prompt = \"Context:\\n\"\n",
    "    for result in search_result:\n",
    "        prompt += result.payload['text'] + \"\\n---\\n\"\n",
    "    prompt += \"Question:\" + query + \"\\n---\\n\" + \"Answer:\"\n",
    "\n",
    "    #print(\"----PROMPT START----\")\n",
    "    #print(\":\", prompt)\n",
    "    #print(\"----PROMPT END----\")\n",
    "    \n",
    "    \n",
    "    completion = ollama.chat(model='llama3.1:8b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': prompt,\n",
    "  },\n",
    "])\n",
    "\n",
    "\n",
    "    return completion[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3c3c12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main idea of the new team's setup is to adopt a flat hierarchy, share responsibilities among team members, and create an open and blame-free environment where everyone has a sense of ownership and stake in the process.\n"
     ]
    }
   ],
   "source": [
    "input = \"what is the main idea of the new teams setup? \"\n",
    "answer = create_answer_with_context(input)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
